{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Hannah E. Schmidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files that get created in this script\n",
    "df_for_biclustering_path = \"<path to file>\" #dataframe for biclustering\n",
    "for_pheno_enrich_path = \"<path to file>\" #og: for_loci_enrichment\n",
    "for_loci_enrich_path = \"<path to file>\"\n",
    "overview_CoCluster = \"<path to file>\" \n",
    "final_pheno_loci_df_path = \"<path to file>\" \n",
    "\n",
    "# BiLouvain \n",
    "bicluster_path = \"<path to directory>\" #path where output files from biclustering can be found\n",
    "\n",
    "# files are output from gene enrichment analysis\n",
    "enrichment_results_loci = \"<path to file>\"  #output from gene_enrichment.R\n",
    "enrichment_results_pheno = \"<path to file>\" #outpuf from gene_enrichment.R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Create file for BiLouvain-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import files \n",
    "\n",
    "#file containing combined GWAS results, detailed association mapping\n",
    "loci_file = \"<path to file>\" \n",
    "\n",
    "#file containing LDSC results, containing genetic correlation and heritability analysis\n",
    "phenotypes_file = \"<path to file>\" \n",
    "\n",
    "# file in \"matrix format\", contains all phenotypes and all loci, if association in GWAS was found 1 otherwise 0\n",
    "matrix_file = \"<path to file>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from loci and phenotype file\n",
    "df_pheno = pd.read_csv(phenotypes_file, sep=\"\\t\")\n",
    "df_loci = pd.read_csv(loci_file, sep=\"\\t\")\n",
    "\n",
    "## derive from matrix file the phenotype names and locis \n",
    "def create_array_from_line(line):\n",
    "    arrays = []\n",
    "    for entry in line.split(\" \"):\n",
    "        if \"L\" in entry:\n",
    "            #print(\"L\", entry)\n",
    "            loci = entry.replace('\"', '')\n",
    "        elif \"\\n\" in entry:\n",
    "            arrays.append(int(entry.split(\"\\n\")[0]))\n",
    "    \n",
    "        else:\n",
    "            arrays.append(int(entry))\n",
    "    return arrays, loci\n",
    "\n",
    "f = open(matrix_file)\n",
    "loci = []\n",
    "for count, line in enumerate(f.readlines()):\n",
    "    if count == 0:\n",
    "        #get names of phenotypes\n",
    "        phenotypes = line\n",
    "    else:\n",
    "        new_array, locus_name = create_array_from_line(line)\n",
    "        if count == 1:\n",
    "            matrix = np.array([new_array])\n",
    "        else:\n",
    "            matrix = np.append(matrix, [new_array], axis=0)\n",
    "        loci.append(locus_name)\n",
    "\n",
    "f.close()\n",
    "phenotypes = [pheno.replace('\"', '') for pheno in phenotypes.split(\" \")]\n",
    "phenotypes = [pheno.replace('\\n', '') for pheno in phenotypes]\n",
    "\n",
    "print(\"Number of phenotypes:\", np.unique(len(phenotypes)))\n",
    "print(\"Number of Loci:\", np.unique(len(loci)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modify dataframe, only keep columns of interest\n",
    "\n",
    "locus_df_cols = [\"locus_id.r2\", \"id\",\"ID\",\"category\",\"phecode\", \"phenotype\", \"locus_id\", \"alleleA\", \"alleleB\", \"v2g.top\",\"study_id.ot\", \"trait_reported.gwas\", \"closest.gene.body\", \"closest.gene.tss\", \"closest.genes\", \"candidate.gene\", \"candidate.gene.locus.r2\"]\n",
    "\n",
    "df_loci_info = pd.DataFrame()\n",
    "for locus_id in loci:\n",
    "    res = df_loci[df_loci[\"locus_id.r2\"]== locus_id[2:]][locus_df_cols]\n",
    "    df_loci_info = pd.concat([df_loci_info, res])\n",
    "\n",
    "df_loci_info.reset_index(drop=True, inplace=True)\n",
    "print(\"created df_loci_info dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biclustering = df_loci_info[[\"locus_id.r2\", \"id\"]].reset_index(drop=True)\n",
    "\n",
    "#save dataframe for biclustering tool\n",
    "df_biclustering.to_csv(df_for_biclustering_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 run BiLouvain Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 create dataframe from BiLouvain Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import biLouvain files\n",
    "\n",
    "\n",
    "#bicluster_path: path where output files from biclustering can be found\n",
    "cluster_dict = bicluster_path + \"reformated_links_for_biclustering_bipartite_Dictionary.txt\"\n",
    "\n",
    "cocluster_res = bicluster_path + \"reformated_links_for_biclustering_bipartite_ResultsCoClusterCommunities.txt\"\n",
    "\n",
    "communities_res = bicluster_path + \"reformated_links_for_biclustering_bipartite_ResultsCommunities.txt\"\n",
    "\n",
    "#init_communities: all loci connected to one phecode build one community, or one loci connected to different phenocodes\n",
    "init_communities = bicluster_path + \"reformated_links_for_biclustering_bipartite_InitialCommunities.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in file\n",
    "bicluster_dict = pd.read_csv(cluster_dict, sep=\"\\t\", header=None)\n",
    "bicluster_dict.rename(columns={0: \"id\", 1: \"vertices\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df of bilouvain community result\n",
    "f = open(communities_res)\n",
    "entries = []\n",
    "cluster_ids = []\n",
    "for count, line in enumerate(f.readlines()):\n",
    "    if line == \"\\n\":\n",
    "        break\n",
    "    for entry in line.split(\":\")[1].split(\",\"):\n",
    "        if entry[0] == \" \":\n",
    "            entry = entry[1:]\n",
    "        \n",
    "        if \"\\n\" in entry:\n",
    "            entries.append(entry.split(\"\\n\")[0])\n",
    "        else:\n",
    "            entries.append(entry)\n",
    "        cluster_ids.append(count)   \n",
    "\n",
    "f.close()\n",
    "\n",
    "# each locus and phenotype is assigned one cluster/community id\n",
    "df_communities = pd.DataFrame({\"entry\": entries, \"Community\": cluster_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df of bilouvain CoCluster result\n",
    "from_vertices_v1 = []\n",
    "to_vertices_v1 = []\n",
    "from_vertices_v2 = []\n",
    "to_vertices_v2 = []\n",
    "f = open(cocluster_res)\n",
    "for count, line in enumerate(f.readlines()):\n",
    "    if line[0:2] == \"Co\":\n",
    "        good_split = line.split(\"\\n\")[0].split(\":\")[1].split(\"-\")\n",
    "        from_vertices = int(good_split[0].split(\"(\")[1][:-1])\n",
    "        to_vertices = int(good_split[1])\n",
    "        if good_split[0][0:2] == \"V1\":\n",
    "            from_vertices_v1.append(from_vertices)\n",
    "            to_vertices_v1.append(to_vertices)\n",
    "        if good_split[0][0:2] == \"V2\":\n",
    "            from_vertices_v2.append(from_vertices)\n",
    "            to_vertices_v2.append(to_vertices)\n",
    "f.close()\n",
    "\n",
    "# checking if V1 - CoClustering is the same as V2\n",
    "df_v1 = pd.DataFrame({\"Phenotype Cluster V1\": to_vertices_v1, \"Loci Cluster V1\": from_vertices_v1})\n",
    "df_v2 = pd.DataFrame({\"from_Cluster V2\": from_vertices_v2, \"to_Cluster V2\": to_vertices_v2})\n",
    "\n",
    "\n",
    "sort_v1 = df_v1.sort_values(by=\"Phenotype Cluster V1\")\n",
    "sort_v1.rename(columns={\"Loci Cluster V1\": \"col2\", \"Phenotype Cluster V1\": \"col1\"}, inplace=True)\n",
    "\n",
    "sort_v2 = df_v2.sort_values(by=\"from_Cluster V2\")\n",
    "sort_v2.rename(columns={\"from_Cluster V2\": \"col1\", \"to_Cluster V2\": \"col2\"}, inplace=True)\n",
    "\n",
    "sort_v1.reset_index(drop=True, inplace=True)\n",
    "sort_v2.reset_index(drop=True, inplace=True)\n",
    "sort_v1.compare(sort_v2) ## the dataframe are the exact same - therefore we can use df_v1 or df_v2 to get information about CoCluster\n",
    "\n",
    "# adding +1 to CoCluster ids, more understandable\n",
    "df_v1[\"CoCluster ID\"] = [x+1 for x in df_v1.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bilouvain clustering information (CoCluster_ID, phenotypes in cluster and loci in cluster) to df_loci_info dataframe\n",
    "\n",
    "pheno_cluster = []\n",
    "loci_cluster = []\n",
    "co_cluster = []\n",
    "for index, entry in df_loci_info.iterrows():\n",
    "    pheno_id = entry.id\n",
    "    locus_id = entry[\"locus_id.r2\"]\n",
    "    \n",
    "    pheno_cluster_num = df_communities[df_communities[\"entry\"]==pheno_id].Community.values[0]\n",
    "    pheno_cluster.append(pheno_cluster_num)\n",
    "    loci_cluster_num = df_communities[df_communities[\"entry\"]==locus_id].Community.values[0]\n",
    "    loci_cluster.append(loci_cluster_num)\n",
    "    \n",
    "    # if the connection of loci-cluster to phenotype-cluster given save CoCluster ID, else save nan \n",
    "    try:\n",
    "        co_cluster.append(df_v1[(df_v1[\"Phenotype Cluster V1\"]==pheno_cluster_num) & (df_v1[\"Loci Cluster V1\"]==loci_cluster_num)][\"CoCluster ID\"].values[0])\n",
    "    except:\n",
    "        co_cluster.append(np.nan)\n",
    "\n",
    "df_loci_info[\"CoCluster_ID\"] = co_cluster\n",
    "df_loci_info[\"phenotype_cluster\"] = pheno_cluster\n",
    "df_loci_info[\"locus_cluster\"]= loci_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for enrichment analysis & further analysis create dataframe which does not include nan's in column: CoCluster_ID \n",
    "df_loci_info_all = df_loci_info\n",
    "\n",
    "df_loci_info = df_loci_info.dropna(subset=[\"CoCluster_ID\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if all loci with same id are in the same cluster\n",
    "for locus_id in np.unique(df_loci_info[\"locus_id.r2\"]):\n",
    "    loci_cluster_ = []\n",
    "    for index, entry in df_loci_info[df_loci_info[\"locus_id.r2\"]==locus_id].iterrows():\n",
    "        #print(entry)\n",
    "        loci_cluster_.append(entry[\"locus_cluster\"])\n",
    "    if len(np.unique(loci_cluster_)) != 1:\n",
    "        print(locus_id)\n",
    "\n",
    "#-> nothing printed out: therefore, all loci with the same id in one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrichment analysis within CoClusters\n",
    "# gprofiler uses dataframe, which needs Cocluster and candidate genes within this cluster (provided in df_loci_info) \n",
    "\n",
    "## uncomment if you want to save\n",
    "df_loci_info.to_csv(for_loci_enrich_path, index=False)\n",
    "df_loci_info_all.to_csv(for_pheno_enrich_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create overview of CoCluster dataframe (containing: phenotype, phenotype description, gene names and locus ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_information_from_df(df, cluster_id, column_to_match_ids):\n",
    "    pheno_category = []\n",
    "    pheno_description = []\n",
    "\n",
    "    pheno_ids = []\n",
    "    locus_ids = []\n",
    "    \n",
    "    gene_names = []\n",
    "    \n",
    "    for index, row in df[df[column_to_match_ids]==cluster_id].iterrows():\n",
    "        pheno_ids.append(row[\"id\"])\n",
    "        pheno_category.append(row[\"category\"])\n",
    "        pheno_description.append(row[\"phenotype\"])\n",
    "        locus_ids.append(row[\"locus_id.r2\"])\n",
    "\n",
    "        if not pd.isnull(row[\"candidate.gene.locus.r2\"]):\n",
    "            split = row[\"candidate.gene.locus.r2\"].split(\"|\")\n",
    "            if len(split) > 1:\n",
    "                for entry in split:\n",
    "                    print(entry)\n",
    "                    gene_names.append(entry)\n",
    "            else:\n",
    "                gene_names.append(split[0])\n",
    "    \n",
    "    gene_names = np.unique(gene_names)        \n",
    "\n",
    "    return(pheno_ids, locus_ids, pheno_category, pheno_description, gene_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotypes_description = []\n",
    "\n",
    "phenotypes_as_string = []\n",
    "num_of_phenotypes = []\n",
    "\n",
    "category_names = []\n",
    "num_of_categories = []\n",
    "\n",
    "loci_as_string = []\n",
    "num_of_loci = []\n",
    "\n",
    "loci_clusters_ids = []\n",
    "pheno_clusters_ids = []\n",
    "\n",
    "num_of_genes = []\n",
    "genes_string = []\n",
    "\n",
    "for cocluster_id in np.unique(df_loci_info[\"CoCluster_ID\"]):\n",
    "    \n",
    "    pheno_ids, locus_ids, pheno_category, pheno_description, gene_names = retrieve_information_from_df(df_loci_info, cocluster_id, \"CoCluster_ID\")\n",
    "    \n",
    "    pheno_ids_string =  \",\".join(str(x) for x in np.unique(pheno_ids)) \n",
    "    num_of_phenotypes.append(len(np.unique(pheno_ids)))\n",
    "    phenotypes_as_string.append(pheno_ids_string)\n",
    "    \n",
    "    \n",
    "    pheno_category_string = \",\".join(str(x) for x in  np.unique(pheno_category))\n",
    "    category_names.append(pheno_category_string)\n",
    "    num_of_categories.append(len(np.unique(pheno_category)))\n",
    "    \n",
    "    \n",
    "    pheno_description = \",\".join(str(x) for x in np.unique(pheno_description))\n",
    "    phenotypes_description.append(pheno_description)\n",
    "    \n",
    "    locus_ids_string = \",\".join(str(x) for x in np.unique(locus_ids))\n",
    "    loci_as_string.append(locus_ids_string)\n",
    "    num_of_loci.append(len(np.unique(locus_ids)))\n",
    "\n",
    "    loci_clusters_ids.append(df_loci_info[df_loci_info[\"CoCluster_ID\"]==cocluster_id][\"locus_cluster\"].values[0])\n",
    "    pheno_clusters_ids.append(df_loci_info[df_loci_info[\"CoCluster_ID\"]==cocluster_id][\"phenotype_cluster\"].values[0])\n",
    "    \n",
    "    \n",
    "    num_of_genes.append(len(gene_names))\n",
    "    genes_string.append(\"|\".join(gene_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_table = pd.DataFrame({\"CoCluster ID\": np.unique(df_loci_info[\"CoCluster_ID\"]), \n",
    "              \"Phenotype\": phenotypes_as_string, \n",
    "              \"Phenotype Description\": phenotypes_description,\n",
    "              \"Category of Phenotypes\": category_names,\n",
    "              \"Loci\": loci_as_string,\n",
    "              #\"Loci Description\": locis_description,\n",
    "              \"Number of Phenotypes\": num_of_phenotypes,\n",
    "              \"Number of Loci\": num_of_loci,\n",
    "              \n",
    "              \"Number of categories\": num_of_categories,\n",
    "              \"Candidate genes\": genes_string,\n",
    "              \"Number of candidate genes\": num_of_genes,\n",
    "              \"Phenotype Community ID\": pheno_clusters_ids,\n",
    "              \"Loci Community ID\": loci_clusters_ids,   \n",
    "             })\n",
    "\n",
    "## uncomment if you want to save\n",
    "sup_table.to_csv(overview_CoCluster, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 run R script for gene enrichment analysis (gprofiler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Analysis gene enrichment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(enrichment_results_loci, sep=\"\\t\")\n",
    "df_pheno = pd.read_csv(enrichment_results_pheno, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring together loci & pheno enrichment results\n",
    "\n",
    "df_new = pd.DataFrame\n",
    "for index, row in df_v1.iterrows():\n",
    "    loci_cluster_info = df[df[\"Cluster\"]== row[\"Loci Cluster V1\"]]\n",
    "    pheno_cluster_info = df_pheno[df_pheno[\"Cluster\"]== row[\"Phenotype Cluster V1\"]]\n",
    "    #cocluster_line = row[\"CoCluster ID\"]\n",
    "    t = pd.concat([loci_cluster_info.drop(columns=[\"Cluster\", \"query\"]).reset_index(), pheno_cluster_info.drop(columns=[\"Cluster\", \"query\"]).reset_index()], ignore_index=True)\n",
    "    \n",
    "    # remove all duplicates here (so that information is only present once)\n",
    "    t = t.sort_values(by=\"p_value\", ascending=False).drop_duplicates(subset=[\"term_name\"], keep=\"first\")\n",
    "    \n",
    "    cocluster_line = np.repeat(row[\"CoCluster ID\"], len(t))\n",
    "    t[\"CoCluster ID\"] = cocluster_line\n",
    "\n",
    "    if not df_new.empty:\n",
    "        df_new = pd.concat([df_new, t])\n",
    "    else:\n",
    "        df_new = t\n",
    "\n",
    "df_new = df_new.drop(columns=[\"index\"]).reset_index(drop=True) # contains all pheno and loci enrichments (that could be found in each Phenotype and Loci Cluster) per CoCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(final_pheno_loci_df_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
